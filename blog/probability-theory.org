#+TITLE:       学习: 刘嘉-概率论22讲
#+AUTHOR:      lijigang
#+EMAIL:       i@lijigang.com
#+DATE:        2020-10-09 Fri
#+URI:         /blog/%y/%m/%d/learn-probability-theory
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:nil \n:nil ::t |:t ^:nil -:nil f:t *:t <:t

#+begin_quote
概率论解决随机问题的本质, 就是把局部的随机性转变为整体上的确定性.

一座城市, 婴儿会在哪一刻诞生是随机的, 但从整体上看, 这座城市的出生率却是大致确定的.

概率论不是帮你预测下一秒会发生什么, 而是为你刻画世界的整体确定性.
#+end_quote

* 概率论的四块基石
** 基石一: 随机
#+begin_quote
随机就是它发生的结果不能被预测.
#+end_quote

随机性是指结果集已知, 但不知下一次会出现哪个结果.

不确定性是指结果集未知.

概率论面对和处理的是随机性, 而不是不确定性.

** 基石二: 概率
 #+begin_quote
 概率是随机事件发生可能性大小的定量描述.
 #+end_quote

什么是随机事件?

任何你关心的事情, 只要 =设定一个条件=, =从可能性的角度出发=, =对某一个发生结果进行陈述=, 就可以转化成随机事件, 然后度量概率.

1. 设定一个条件

   你不能计算 *人类登上火星的概率*, 但你加上时间限定后就可以计算, 比如计算 *人类在 2050 年登上火星的概率*.

2. 从可能性的角度出发

   只要你还不确定结果, 就可以从可能性的视角提出问题, 比如计算 *明天下雨的概率*, 或者计算 *你家地底下有石油的概率*.

3. 对某一个发生结果的陈述

   陈述的必须是一个随机结果, 而不是不确定性.


如何计算随机事件发生的概率?

*随机事件占样本空间的比率, 就是这个随机事件发生的概率.*

扔骰子, 样本空间是 1,2,3,4,5,6 共六个结果, 扔到 1 点的概率, 就是 1/6.
** 基石三: 独立性
如果随机事件之间没有任何关联, 我们就说这些随机事件是相互独立的, 它们之间就具备独立性. 而这种具备独立性的随机事件, 被称为 =独立事件=.

两个随机事件相互独立, 在概率论中说的就是, 一个随机事件的发生, 不影响另一个随机事件发生的概率.

扔硬币看结果是正还是反面, 就是一个典型的独立事件.

前三次连续扔的结果都是正面朝上, 第四次的结果和前三次无关, 正面朝上的概率仍为 1/2.
** 基石四: 概率计算
再复杂的概率问题, 都是基于三个计算法则.
1. 排列组合法则

   排列组合法则适用于结果有限，而且每种结果都是 =等可能性= 的情况.

   计算方式就是这个随机事件出现的次数除以所有可能的结果的个数.

   比如计算扔骰子扔到 2 的概率, 可能出现的结果是 1 到 6 共 6 个可能, 2 是其中一个结果, 概率就是 1/6.

   再比如, 假设生男生女是等概率的, 朋友家有两个小孩, 问都是男孩的概率是多少?

   全部的结果是四种:男男、男女、女男、女女, 男男的概率是 1/4.
2. 加法法则

   排列组合法则是针对单个随机事件的概率计算, 加法法则针对的是多个随机事件.

   N 个随机事件是互斥的, 那么发生其一的概率, 等于这 N 个随机事件各自发生的概率的和.

   比如已知你得到冠军的概率为 10%, 得到亚军的概率为 30%. 得到冠军和亚军是互斥的, 只能为其一. 那么你进入决赛的概率为 10%+30% = 40%.

3. 乘法法则

   乘法法则也是针对多个随机事件的概率计算.

   N 个 =独立事件= 同时发生的概率, 即为将 N 个随机事件各自发生的概率相乘.
** 三种度量概率方法
常用的度量概率的方法有三种: 定义法、频率法、迭代法.

定义法现在用的比较少了, 频率法和迭代法到现在还在大量使用.

1. 定义法

   #+begin_quote
   定义法就是直接定义, 直接认为某件事不同结果出现的可能性是相等的.
   #+end_quote

   比如定义法认为, 抛硬币正面朝上和反面朝上的概率, 都是 50%; 扔骰子每个点数向上的概率也相等, 都是 1/6.

   定义法虽然简单粗暴, 但在宏观尺度上, 它是一种对现实世界的合理简化, 有一定的科学性.

2. 频率法

   #+begin_quote
   频率法是指, 只要数据量足够大, 一个随机事件发生的频率就会无限接近它发生的概率.
   #+end_quote

   也就是说, 虽然每次结果是随机的, 但只要重复的做这件事, 重复足够多次数, 隐含的规律就会浮现出来.

3. 迭代法

   但如果没有办法获得大量数据, 或者根本就不能反复试验怎么办?

   #+begin_quote
   迭代法是指, 先利用手头少量的数据做推测, 甚至是主观猜测一件事儿的概率, 然后再通过收集新的数据, 不断调整对这件事概率的估算.

   最常用的方法就是 =贝叶斯=.
   #+end_quote

   比如, 虽然没办法反复试验来判断人类 2050 年登上火星的概率, 但是我可以先假设概率是 50%, 然后再根据各种新的信息(登陆计划的公开、核心计算的发展状态、经费变化等等), 来调整之前的预测.



定义法，是通过自然世界的对称性来定义概率；

频率法，是用随机事件发生的频率来计算概率；

而迭代法，是用一种动态发展的、考虑个人差异的角度来度量概率

* 频率法
#+begin_quote
只要重复的试验或者观测的数据 =足够多= ，随机事件发生的频率就会无限接近它的概率。

这就是我们现在常说的“大数定律”.
#+end_quote

** 接近
大数定律告诉我们，数据或者试验越多，频率就会越接近概率。当然，只是接近，在真实概
率上下浮动。这种浮动的范围就是“精度误差”。比如，前面那些数学家们抛硬币的结果，并
不是刚好等于理想的 50%，这个和理想值之间的差距，就是精度误差。假如你抛出正面朝上
的频率是在 47%-53%之间，那么精度误差就是+-3%.

针对这+-3%的误差率，我做 100 组试验，或者统计学上叫 100 组样本，如果有 95 组样本算出来
的频率，正好在这个精度误差的范围之内，我们就称之为 95%的“置信度”.

通过精度误差和置信度的限定, 容忍一定错误的发生, 我们在用频率度量概率时, 可以大幅减少试验的次数或采集的数据量.

** 足够多
只有数据量足够多的时候，局部频率才会接近真实概率。

当数据量很少的时候，一件事发生的频率可能和它的真实概率相差很大.

#+begin_quote
均值回归是指，如果一个数据和它的正常状态偏差很大，那么它向正常状态回归的概率就会
变大.
#+end_quote

比如一个学生的数学水平是 70 分, 这次超水平发挥考了 100 分, 下一次考试, 他大概率考不到 100 分, 但可能考 90 分, 可能考 80 分, 也可能考 70 分. 这些都比 100 分正常, 都更接近他的真实水平, 所以都是均值回归.

而不会说, 这次考 100 分, 下次只能考 50 分来补偿上次的高分.

*总之，大数定律不需要补偿，而是通过均值回归，通过产生大量的正常数据，削弱之前异常
 数据的影响*.

** 数学期望: 长期价值的数字化衡量
#+begin_quote
数学期望简称期望，计算方法很简单，就是对随机事件不同结果的概率加权求平均。

用大白话说就是，先把每个结果各自发生的概率和带来的影响相乘，然后把算出来的数相加。
最后算出来的结果就是数学期望.
#+end_quote

篮球有三种得分方式——篮下、中距离和三分球。篮下进攻和中距离投中都是 2 分，而三分球
距离更远，投中得 3 分。当然，距离越远投篮命中率一般就会越低。总之，篮下投篮命中率
高，但是得分低，三分投篮命中率低，但是得分高。哪种得分方式更有效率呢?

更有效率，这是一个长期价值。而一旦判断一件事的长期价值，数学期望就派上用场了。每
种得分方式的数学期望，可以用得分情况和平均命中率来计算。具体来说，篮下每命中一球
得 2 分，如果平均命中率是 55%，那计算一下，篮下出手的数学期望就是 1.1 分.

这 1.1 分是什么意思呢？它的意思是说，长期来看，平均每次篮下进攻可以得到 1.1 分。数学
期望，就是用来衡量这种长期的平均价值的。

类似的，中距离投篮也是 2 分，球员的平均命中率是 45%，那中距离投篮的数学期望就是 0.9
分。三分球是 3 分，平均命中率是 35%，那三分球投篮的数学期望就是 1.05 分。

*数学期望是衡量一件事的价值，判断一件事值不值得做的重要指标.*

** 方差: 围绕数学期望波动程度的度量
数学期望相同，并不代表两件事的价值就一样。

随机结果的波动程度，同样对一件事的价值，对我们的决策影响巨大.

#+begin_quote
方差描述的就是，随机结果围绕数学期望的波动范围。
#+end_quote

方差越大，说明这件事波动性越大。而风险，本质上指的就是波动性。

所以， =方差的本质，就是对风险的度量.=

一个随机事件的方差越大，可能的结果离期望值越远，就说明它的风险越大.

彩票就是一个方差极大的生意, 支付宝的集五福活动就是一个方差很小的活动.

** 概率分布: 上帝视角看整体轮廓
什么是随机变量呢？简单说，就是把随机事件可能的结果抽象成一个数字，每个数字对应一个概率。这个随机变化的数字，就是随机变量.

有了随机变量，我们就把现实世界和数学世界打通了。要寻找一个随机事件的规律，直接分析随机变量这个具体数字的变化情况就好了.

把随机变量所有的结果和它对应的概率全部统计出来后，我们就有了一个东西——概率分布。

*概率分布模型是我们对现实规律的抽象.*

现实世界里越来越多的随机变量的变化规律，被数学家发现。也就是说，我们的概率分布模
型越来越多。有了这些模型，解决各种随机事件就简单多了，看看它适用于哪个模型，直接
带入公式计算就好了.

** 正态分布

在正态分布的曲线图里，横坐标代表随机变量的取值范围，越往右，随机变量的值就越大；
纵坐标，则代表概率的大小，最底下的概率是 0，越往上概率越大。这样，从曲线上随便找
一点，确定它的横坐标、纵坐标，我们就知道了这个值出现的概率是多少。

正态分布有三个性质:
1. 均值就是期望
2. 极端值很少
3. 标准差决定胖瘦


数学期望代表长期价值，而现在平均值又是数学期望。也就是说，在正态分布中，平均值就
 代表随机事件的价值. 在正态分布里，平均值才具有这样的意义。如果不是正态分布，均
 值可能就没啥意义了。比如说地震，谁也没听说过平均强度和平均损失这样的说法吧？

为什么说正态分布简单？就是因为在正态分布中，平均值等于期望，决定这条曲线的最高点；
方差决定胖瘦，决定曲线的弯曲度。简单两个数据，就确定了这条曲线的形状。

只要是互联网用户, 一定会经常遇到这样的提示信息:  “你的开机时间 23 秒，打败了全国 97%的用户”。 你有没有想过他是怎么知道你打败了多少用户的?

把全国每台电脑的开机时间都收集起来做排序, 当然可以实现, 但这个太复杂了.

电脑开机时间这事是符合正态分布的. 只要随机抽取一小部分用户的开机数据, 算出均值和标准差, 就可以确定这条正态分布曲线.

有了这条曲线, 当你电脑开机的时候, 只需要比较你的开机时间和均值的差距, 就能知道你距离均值多少个标准差, 也就知道你的排名了.

其他人总是用“刻意练习”“精准”等来评价专业和业余，但在数学家看来，这些词都太模糊。

真正精确的标准只有两个——均值和标准差。

专业就是均值更高，标准差更小，业余恰恰相反.
** 中心极限定理
#+begin_quote
大量独立的随机变量相加，无论各个随机变量的分布是怎样的，它们相加的结果必定会趋向于正态分布.
#+end_quote

因为任何分布叠加最终都会形成正态分布，所以无论是对数分布还是幂律分布，无论是指数
分布还是其他任何分布，只要自身不断演化，不断自己叠加自己，最终也一样会变成正态分
布。

或许我们可以这么说，所有的分布，不是正态分布，就是在变成正态分布的路上.

在中心极限定理之后，信息论领域发现了“熵最大原理”。也就是说，在一个孤立系统中，熵总是在不断增大。

而巧合的是，正态分布就是所有已知均值和方差的分布中，信息熵最大的一种分布。

如果熵不断增长是孤立系统确定的演化方向，那熵的最大化，也就是正态分布，就是孤立系
统演化的必然结果。
** 幂律分布

横坐标，代表随机变量的取值；纵坐标，代表发生的概率。而幂律分布就是一条向下的曲线，
拖着一个长长的尾巴。它的含义也非常明确——在随机变量中，越小的数值，出现的概率越大；
越大的数值，出现的概率则越小.

幂律分布让原本不会发生的极端事件发生.在数学上，这个叫“长尾”，也叫肥尾、厚尾。简
单说就是，虽然极端数据出现的概率很低，但这个概率永远不会趋近于 0，永远不会小到可
以忽略不计。

这也和正态分布不同。在正态分布里，数据非常集中，非常极端的数据几乎不可能出现，可以直接忽略不不计。而在幂律分布里，再极端的数据都有出现的可能.

正态分布是一种均匀对称分布，大多数数据都集中在平均值附近，所以平均值非常有用，因
为它代表大多数。而幂律分布呢？它的数据变化幅度非常大，平均值毫无意义。拿个人收入
来说，有一贫如洗的穷人，也有挥金如土的富豪，把这两群人的资产平均，完全没有意义.
** 泊松分布
#+begin_quote
泊松分布是用来描述随机事件发生次数的概率的一种分布.

计算公式就是: 随机事件发生 k 次的概率，等于 lambda 的 k 次方除以 k 的阶乘，再乘以自然底数 e 的负 lambda 次方.
#+end_quote

比如我们经常听到气象部门说, 遇到了 50 年一遇的大暴雨. 你想知道接下来 50 年内,  发生 K 次这级别暴雨的概率, 可以把 K 值代入到公式计算.

K=0, 1 次都不发生的概率, 是 37%.

k=1, 发生 1 次的概率, 是 37%.

k=2, 发生 2 次的概率, 是 18%.

泊松分布具有两个重要特性:
1. 基础是正态分布
2. 随机事件的间隔是无记忆的
* 迭代法
** 条件概率
#+begin_quote
所谓的条件概率，通俗来讲就是，如果一个随机事件的概率会因为某个条件而产生变化，那
在这个条件发生的情况下，这个随机事件发生的概率就是条件概率.
#+end_quote

公式计算为:
P(A│B) = P(AB)/P(B)

其中，P(A│B)为条件概率，表示在 B 条件下 A 发生的概率；

P(AB)为事件 A、B 同时发生的概率；

P(B)为事件 B 发生的概率.

很多条件概率是很隐蔽的. 现在中, 所有的概率本质上都是条件概率.

条件概率量化了条件对随机事件的影响, 但它只表示统计意义上的相关性, 并不代表因果关系.
** 贝叶斯推理: 概率是对信心的度量
#+begin_quote
根据新信息不断调整对一个随机事件发生概率的判断，这就是贝叶斯推理。
#+end_quote

在贝叶斯的世界里，概率本质上是对信心的度量，是我们对某个结果相信程度的一种定量化的表达.

比如我们说明天下雨的概率是多少、这个客户我能拿下的概率是多少、凶手是某某某的概率是多少时，也都是在表达一种信心.

贝叶斯推理这种思维方式有两大优势:
1. 起点不重要, 迭代很重要
2. 信息越充分, 结果越可靠
** 贝叶斯计算
#+begin_quote
贝叶斯公式的数学表达是——

P（A|B）= P（B|A）x P（A）/ P（B）

翻译过来就是，现象 B 出现的情况下事件 A 发生的概率，等于事件 A 发生时现象 B 出现的概率，乘以事件 A 发生的概率，再除以现象 B 出现的概率.
#+end_quote

P(A)是先验概率, 你可以任性设置, 但最好遵循三点:
1. 相信历史数据
2. 参考专家意见
3. 平均设置概率

P（B|A）和 P(B)这两个数一定是客观的，必须找到具体的客观值，而不能拍脑袋随便设定.

贝叶斯计算难度不是在计算本身，而是寻找调整因子的客观数据.

* 概率思维
频率学派和贝叶斯学派都是完全正确、完全有效的.

频率法和贝叶斯最大的差异就是两个方法的假设不一样。

频率法，更像是做题，必须有明确的、严格的前提约束，严格界定好所有的条件。它假设信息是全知的，每道题都有一个对所有人而言都正确的答案。所以会通过反复的试验，不断逼近最终那个客观概率。过程不重要，达到最终那个客观的结果才重要。

而贝叶斯，是个动态的、反复的过程。每个新信息的加入都要重新进行一遍计算，获得一个新概率。贝叶斯没有什么限制条件，只是在这一次次获得新信息、重新计算的过程中迭代自己的判断。它甚至不认为现实的事儿都有正确答案，因为所谓答案，也是在不断变化的.

除了掌握频率法和迭代法两大门派的武功招式以外, 还需要掌握一些内功心法才行.

概率思维有三大黄金原则, 分别是:
** 原则一: 对抗直觉, 能算就算

很多概率相关的事情，不要相信自己的直觉，只要动笔简单算一算，就很容易得出结论.

行为经济学、认知科学里有很多反直觉的套路。什么是反直觉？本质上就是直觉错判了一件
事的概率。这个时候，我们要做的就是遏制直觉的冲动，去寻找数据、寻找证据，用概率公
式计算一下。通过计算对抗直觉，你就拥有了一个概率专家的基本素养.
** 原则二: 寻找条件, 增大概率
生活中，几乎所有涉及个体的决策都是如此。想要成功，就要找到对自己成功影响最大的那
个条件概率。换句话说，想要成功，就是找到最大化概率的条件。

对于创业来说，成功的平均概率可能只有 1%，但如果你拥有关键技术、找到了蓝海、采取
了差异化竞争策略，你成功的概率就会大大增加。对于工作来说，想要搞定客户，就要寻找
在什么条件下客户会最满意。还有，互联网行业的增长黑客，其实就是通过数据去寻找导致
转化的各种条件，从而提高产品的转化率.
** 原则三: 相信系统, 长期主义

所谓的科学决策，其实是一个决策系统，只要决策系统有概率优势，我们就要长期坚持，相
信系统，而不在乎单次决策的随机结果的好坏.

你流的每一滴汗，读的每一本书，都会一点点的改变你的身体，改变你的认知。这些微小的
改变，这些微小的概率提升，在时间的作用下都能被无限放大。

站在当下，未来任何事都只是一个概率。所谓坚持，所谓努力，其实就是寻找一个大概率的
方向，然后相信系统，相信长期主义。

当然，你得坚持活着，等到长期的到来.
